Requires Changes

1 SPECIFICATION REQUIRES CHANGES

Hello Udacian,

It is really good of what you did. I do enjoy to review your work. It is amazing. I hope that you love what I comment for you and it helps you to understand more.
Please remember to adjust the Graduation rate. After that, you might pass.
Looking forward to see your next submission.

Keep up with the hard work!
Classification vs Regression

Student is able to correctly identify which type of prediction problem is required and provided reasonable justification.
Well done identifying the type of machine learning problem! This is important, and you should always think about this when attempting to predict a phenomenon.
If you don't think about this beforehand, you might end up wasting a lot of time because you model the prediction task in a wrong way (regression instead of classification or vice-versa).

Well done identifying the type of machine learning problem! This is important, and you should always think about this when attempting to predict a phenomenon.
If you don't think about this beforehand, you might end up wasting a lot of time because you model the prediction task in a wrong way (regression instead of classification or vice-versa).

Suggestions and Comments:

Pro Tip:

Here is a diagram of the Predictive Modelling Process, obtained from this link, just for your reference:
classification01.jpg
Exploring the Data

Student response addresses the most important characteristics of the dataset and uses these characteristics to inform their decision making. Important characteristics must include:

Number of data points
Number of features
Number of graduates
Number of non-graduates
Graduation rate
Not bad! the graduation rate seems not what we expect to have.Below are some of my comments and feedback:

Suggestions and Comments:

You might consider removing the two zeros after 67 in 67.00%, since you are rounding to 67. If you were reporting the precise graduation rate to two decimal places, then you could have included the two decimal points after 67.
Preparing the Data

Code has been executed in the iPython notebook, with proper output and no errors.
Well done here! Your code is short and precise.

Pro Tips:

Mastering Markdown formatting is extremely useful if you want to produce Quality Reports in iPython Notebook.
Please check out this link if you want to learn more about Markdown formatting.
Training and test sets have been generated by randomly sampling the overall dataset.
Your dataset was well split. Below are some of my suggestions and comments regarding splitting of data.

Suggestions and Comments:

For more efficiency, you could consider using the function train_test_split from scikit-learn, as shown in the line below:
X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, train_size=num_train, random_state=42)
It's faster and less prone to errors.
Training and Evaluating Models

The pros and cons of application for each model is provided with reasonable justification why each model was chosen to explore.
Fantastic Job Writing about the 3 Models in details!
Please cite your work, as citations add much more credibility to your work

Below are my comments, thoughts, and what you can improve:

KNN:

The following resources might be very useful for learning more about KNNs:

 - http://www.nickgillian.com/wiki/pmwiki.php/GRT/KNN#Disadvantages
 - https://www.youtube.com/watch?v=UqYde-LULfs
 - http://people.revoledu.com/kardi/tutorial/KNN/Strength%20and%20Weakness.htm
KNNs don't train and store a specific model, so they don't really learn from data. Instead, they classify data by directly dividing the data into the classes in the feature space.
SVM:

If you wanna go deeper with SVMs, check these out:
http://scikit-learn.org/stable/modules/svm.html
https://www.researchgate.net/post/What_is_the_running_time_complexity_of_SVM_and_ANN
http://cs229.stanford.edu/notes/cs229-notes3.pdf
Decision Trees:

SUGGESTED READING

These links helped me a lot when learning about Decision Trees, hope they help you too:
https://www.quora.com/What-are-the-advantages-of-using-a-decision-tree-for-classification
https://www.quora.com/What-are-the-disadvantages-of-using-a-decision-tree-for-classification
http://scikit-learn.org/stable/modules/tree.html#complexity
http://www.cbcb.umd.edu/~salzberg/docs/murthy_thesis/survey/node32.html
All the required time and F1 scores for each model and training set sizes are provided within the chart given. The performance metrics are reasonable relative to other models measured.
Well done here! Below are some comments and feedback just for your reference:

Suggestions and Comments:

Consider using for loops to replace to avoid repeating code:
for clf in [clf_A, clf_B, clf_C]:
  for size in [100, 200, 300]:
      train_predict(clf, X_train[:size], y_train[:size], X_test, y_test)
      print '\n'
Choosing the Best Model

Justification is provided for which model seems to be the best by comparing the computational cost and accuracy of each model.
Great rationale.
You did awesome in this particular section!
Student is able to clearly and concisely describe how the optimal model works in laymen terms to someone what is not familiar with machine learning nor has a technical background.
Your description of SVM in layman’s terms sounds great!
I will suggest that I will be better if you can utilize some graph to explain your answer to a layman, because it is not easy to a layman to know what 'Hyperplane' and 'maximal margin' is.
The final model chosen is correctly tuned using gridsearch with at least one parameter using at least three settings. If the model does not need any parameter tuning it is explicitly stated with reasonable justification.
Great job fine-tuning your algorithm!
The F1 score is provided from the tuned model and performs approximately as well or better than the default model chosen.
You did awesome in this particular section! Since you sound like a pro, below are some pro tips for you:
In addition to my comments above, here are some comments for you:

You did awesome in this particular section! Since you sound like a pro, below are some pro tips for you:
In addition to my comments above, here are some comments for you:

SUGGESTIONS AND COMMENTS:

Pro Tip:

Using Stratified Shuffle Split:

As mentioned in the section about dataset exploring the data, the dataset is quite unbalanced.
To better model unbalanced data, it’s always preferable to use a stratified shuffle split.
This is because using Stratified shuffle split, the dataset is split so as to preserve the percentage of samples for each class.
This method avoids not having a single representative of the minor class in a fold.
Also, given a small data with highly unbalanced classes, stratification provides a safeguard and more consistency among the classes in the two splits.
This paper gives more information on stratified cross-validation. In the paper, it is stated that the main advantage of this procedure is that it reduces experimental variance, which makes it easier to identify the best methods under consideration (hyper-parameters)
Below is a short implementation of StratifiedShuffleSplit from sklearn, using adaboost as the classifier. Note that you could also use other algorithms such as SVMs or Decision Trees to conduct grid search using stratified shuffle split.
# Import StratifiedShuffleSplit from sklearn
from sklearn.cross_validation import StratifiedShuffleSplit
from sklearn.grid_search import GridSearchCV
from sklearn.metrics import make_scorer

# Define a random state to seed your algorithms
ran_state = 201

# Define your classifier to optimise. You can choose to optimise classifiers
# other than adaboost.
clf = AdaBoostClassifier(random_state=ran_state)

# Create the parameters list you wish to tune
parameters = {'n_estimators' : [5, 10, 15, 20],
              #'algorithm'    : ['SAMME', 'SAMME.R'],
              'learning_rate': [0.01, 0.50, 0.60, 0.70]}

# Create the Stratified Shuffle Split object
sss = StratifiedShuffleSplit(y_train, n_iter=10, test_size=0.24, random_state=ran_state)

# Make an f1 scoring function using 'make_scorer'
f1_scorer = make_scorer(f1_score, pos_label="yes")

# Create a grid search object, and use the sss object in place of cv parameter
grid_obj = GridSearchCV(clf, param_grid=parameters, scoring=f1_scorer,
                       cv=sss)

# Fit the grid search object to the training data and find the optimal parameters
grid_obj.fit(X_train, y_train)

# Get the estimator
clf = grid_obj.best_estimator_

# Print the parameters
print clf.get_params(), '\n'

# Report the final F1 score for training and testing after parameter tuning
print "Tuned model has a training F1 score of {:.4f}.".format(predict_labels(clf, X_train, y_train))
print "Tuned model has a testing F1 score of {:.4f}.".format(predict_labels(clf, X_test, y_test))
Quality of Code

Code reflects the description in the documentation.
Well done, your code is very well written Below are some Pro Tips which can help you in becoming a much better programmer:

Pro Tips:

Writing Clean code is very important and is an integral part of being a Machine Learning Engineer. If you wish to master the art of writing clean code in Python, you may want to check out the Google Python Style Guide.
You don’t have to read all the guide at once; you can choose to read only the parts relevant to you at the moment.
